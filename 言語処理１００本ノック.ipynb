{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 言語処理100本ノック\n",
    "http://www.cl.ecei.tohoku.ac.jp/nlp100/#ch1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00. 文字列の逆順\n",
    "文字列\"stressed\"の文字を逆に（末尾から先頭に向かって）並べた文字列を得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'desserts'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_0 = \"stressed\"\n",
    "word_0[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. 「パタトクカシーー」\n",
    "「パタトクカシーー」という文字列の1,3,5,7文字目を取り出して連結した文字列を得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'パトカー'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patatoku = \"パタトクカシーー\"\n",
    "target_1 = \"\"\n",
    "for i in range(4):\n",
    "    target_1 += patatoku[2 * i]\n",
    "target_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. パトカー」＋「タクシー」＝「パタトクカシーー」\n",
    "「パトカー」＋「タクシー」の文字を先頭から交互に連結して文字列「パタトクカシーー」を得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'パタトクカシーー'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat = \"パトカー\"\n",
    "tax = \"タクシー\"\n",
    "target_2 = \"\"\n",
    "for i in range(4):\n",
    "    target_2 += pat[i]\n",
    "    target_2 += tax[i]\n",
    "target_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. 円周率\n",
    "\"Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.\"という文を単語に分解し，各単語の（アルファベットの）文字数を先頭から出現順に並べたリストを作成せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 9]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_3 = \"Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.\"\n",
    "phrase_3 = phrase_3.replace(\",\", \"\") #カンマの除去\n",
    "phrase_3 = phrase_3.replace(\".\", \"\") #ピリオドの除去\n",
    "phrase_3_splited = phrase_3.split()\n",
    "target_list_3 = []\n",
    "for i in range(len(phrase_3_splited)):\n",
    "    target_list_3.append(len(phrase_3_splited[i]))\n",
    "target_list_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. 元素記号\n",
    "\"Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.\"という文を単語に分解し，1, 5, 6, 7, 8, 9, 15, 16, 19番目の単語は先頭の1文字，それ以外の単語は先頭に2文字を取り出し，取り出した文字列から単語の位置（先頭から何番目の単語か）への連想配列（辞書型もしくはマップ型）を作成せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'He', 'Lied', 'Because', 'Boron', 'Could', 'Not', 'Oxidize', 'Fluorine', 'New', 'Nations', 'Might', 'Also', 'Sign', 'Peace', 'Security', 'Clause', 'Arthur', 'King', 'Can']\n",
      "['H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne', 'Na', 'Mi', 'Al', 'Si', 'P', 'S', 'Cl', 'Ar', 'K', 'Ca']\n"
     ]
    }
   ],
   "source": [
    "phrase_4 = \"Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.\"\n",
    "phrase_4 = phrase_4.replace(\",\", \"\") #カンマの除去\n",
    "phrase_4 = phrase_4.replace(\".\", \"\") #ピリオドの除去\n",
    "phrase_4_splited = phrase_4.split()\n",
    "print(phrase_4_splited)\n",
    "\n",
    "list_of_num = [1, 5, 6, 7, 8, 9, 15, 16, 19]\n",
    "target_list_4 = []\n",
    "\n",
    "for i in range(len(phrase_4_splited)):\n",
    "    if i+1 in list_of_num:\n",
    "        target_list_4.append(phrase_4_splited[i][0])\n",
    "    else:\n",
    "        target_list_4.append(phrase_4_splited[i][:2])\n",
    "\n",
    "print(target_list_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_phrase(phrase):\n",
    "    phrase = phrase.replace(\",\", \"\") #カンマの除去\n",
    "    phrase = phrase.replace(\".\", \"\") #ピリオドの除去\n",
    "    phrase = phrase.replace(\": \", \"\")\n",
    "    phrase_splited = phrase.split()\n",
    "    return phrase_splited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. n-gram\n",
    "与えられたシーケンス（文字列やリストなど）からn-gramを作る関数を作成せよ．この関数を用い，\"I am an NLPer\"という文から単語bi-gram，文字bi-gramを得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iam', 'aman', 'anNLPer']\n",
      "['Ia', 'am', 'ma', 'an', 'nN', 'NL', 'LP', 'Pe', 'er']\n"
     ]
    }
   ],
   "source": [
    "phr_5 = \"I am an NLPer\"\n",
    "word_list_5 = phr_5.split()\n",
    "word_series_5 = phr_5.replace(\" \", \"\")\n",
    "\n",
    "word_bigram = []\n",
    "letter_bigram = []\n",
    "\n",
    "for i in range(len(word_list_5) -1):\n",
    "    word_bigram.append(word_list_5[i] + word_list_5[i+1])\n",
    "print(word_bigram)\n",
    "\n",
    "for i in range(len(word_series_5) -1):\n",
    "    letter_bigram.append(word_series_5[i] + word_series_5[i+1])\n",
    "print(letter_bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. 集合\n",
    "\"paraparaparadise\"と\"paragraph\"に含まれる文字bi-gramの集合を，それぞれ, XとYとして求め，XとYの和集合，積集合，差集合を求めよ．さらに，'se'というbi-gramがXおよびYに含まれるかどうかを調べよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iam', 'aman', 'anNLPer']\n",
      "['Ia', 'am', 'ma', 'an', 'nN', 'NL', 'LP', 'Pe', 'er']\n"
     ]
    }
   ],
   "source": [
    "def tango_bigram(phrase):\n",
    "    phrase = phrase.replace(\",\",\"\").replace(\".\",\"\")\n",
    "    word_list = phrase.split()\n",
    "    bigram = []\n",
    "    \n",
    "    for i in range(len(word_list) -1):\n",
    "        bigram.append(word_list[i] + word_list[i + 1])\n",
    "    return bigram\n",
    "\n",
    "def moji_bigram(phrase):\n",
    "    phrase = phrase.replace(\",\",\"\").replace(\".\",\"\").replace(\" \",\"\")\n",
    "    bigram = []\n",
    "    \n",
    "    for i in range(len(phrase) -1):\n",
    "        bigram.append(phrase[i] + phrase[i+1])\n",
    "    return bigram\n",
    "\n",
    "print(tango_bigram(phr_5))\n",
    "print(moji_bigram(phr_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: ['pa', 'ar', 'ra', 'ap', 'pa', 'ar', 'ra', 'ap', 'pa', 'ar', 'ra', 'ad', 'di', 'is', 'se']\n",
      "Y: ['pa', 'ar', 'ra', 'ag', 'gr', 'ra', 'ap', 'ph']\n"
     ]
    }
   ],
   "source": [
    "a = \"paraparaparadise\"\n",
    "b = \"paragraph\"\n",
    "\n",
    "X = moji_bigram(a)\n",
    "Y = moji_bigram(b)\n",
    "print(\"X:\",X)\n",
    "print(\"Y:\",Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "和集合: {'ra', 'gr', 'ph', 'ap', 'ar', 'se', 'di', 'pa', 'ad', 'is', 'ag'}\n",
      "積集合: {'ra', 'pa', 'ar', 'ap'}\n",
      "差集合: {'is', 'ad', 'se', 'di'}\n"
     ]
    }
   ],
   "source": [
    "X_set = set(X)\n",
    "Y_set = set(Y)\n",
    "\n",
    "XY_union = X_set.union(Y_set) #和集合\n",
    "XY_intersection = X_set.intersection(Y_set) #積集合\n",
    "XY_difference = X_set.difference(Y_set) #差集合\n",
    "\n",
    "print(\"和集合:\", XY_union)\n",
    "print(\"積集合:\", XY_intersection)\n",
    "print(\"差集合:\", XY_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. テンプレートによる文生成\n",
    "引数x, y, zを受け取り「x時のyはz」という文字列を返す関数を実装せよ．さらに，x=12, y=\"気温\", z=22.4として，実行結果を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12時の気温は22.4\n"
     ]
    }
   ],
   "source": [
    "def xyz(x, y, z):\n",
    "    print(str(x) + \"時の\" + y + \"は\" + str(z))\n",
    "\n",
    "xyz(12, \"気温\", 22.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. 暗号文\n",
    "与えられた文字列の各文字を，以下の仕様で変換する関数cipherを実装せよ．\n",
    "\n",
    "英小文字ならば(219 - 文字コード)の文字に置換\n",
    "その他の文字はそのまま出力\n",
    "この関数を用い，英語のメッセージを暗号化・復号化せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agyzhs rh z hrnkov hfyhgrgfgrlm xrksvi uli gsv Hvyivd zokszyvg.\n",
      "Atbash is a simple substitution cipher for the Hebrew alphabet.\n"
     ]
    }
   ],
   "source": [
    "string = \"Atbash is a simple substitution cipher for the Hebrew alphabet.\"\n",
    "\n",
    "def cipher(input):\n",
    "    ret = \"\"\n",
    "    for char in input:\n",
    "        ret += chr(219 - ord(char)) if char.islower() else char\n",
    "    return ret\n",
    "\n",
    "string = cipher(string)\n",
    "print(string)\n",
    "string = cipher(string)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09. Typoglycemia\n",
    "スペースで区切られた単語列に対して，各単語の先頭と末尾の文字は残し，それ以外の文字の順序をランダムに並び替えるプログラムを作成せよ．ただし，長さが４以下の単語は並び替えないこととする．適当な英語の文（例えば\"I couldn't believe that I could actually understand what I was reading : the phenomenal power of the human mind .\"）を与え，その実行結果を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"touldn'c\", 'eelievb', 'that', 'I', 'doulc', 'yctualla', 'dnderstanu', 'what', 'I', 'was', 'geadinr', 'the', 'lhenomenap', 'rowep', 'of', 'the', 'numah', 'mind']\n"
     ]
    }
   ],
   "source": [
    "string_7 = \"I couldn't believe that I could actually understand what I was reading : the phenomenal power of the human mind .\"\n",
    "\n",
    "list_of_words = split_phrase(string_7)\n",
    "\n",
    "output = []\n",
    "\n",
    "for i, word in enumerate(list_of_words):\n",
    "    if len(word) > 4:\n",
    "        first = word[0]\n",
    "        medium = word[1:-1]\n",
    "        last = word[-1]\n",
    "        output.append(last + medium + first)\n",
    "    else:\n",
    "        output.append(word)\n",
    "print(output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
